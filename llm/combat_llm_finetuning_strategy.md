# Combat LLM Fine-Tuning Strategy for Harfang RL-LLM

## ðŸŽ¯ **Overview**
This document outlines a comprehensive strategy for fine-tuning LLMs specifically for air combat tactical guidance, leveraging the GPT-4 generated tactical data from the main branch and creating specialized combat-focused models.

---

## ðŸ“Š **Current Data Assets Analysis**

### **Available Training Data (from main branch)**
1. **`data/dcs_gpt4_self_interpretation_dataset.jsonl`** (101 samples)
   - GPT-4 generated tactical scenarios
   - F-16 vs Su-30 engagements
   - Structured action categories: Maneuver, Sensoring, Firing, Countermeasuring

2. **`data/dcs_ai_enhanced_training_data.jsonl`** (101 samples)  
   - Enhanced tactical scenarios
   - Additional context and complexity

3. **`data/doctriner_gpt_generates.py`** (Data generator)
   - Can generate additional scenarios
   - Multiple prompt strategies implemented
   - Configurable scenario complexity

### **Data Quality Assessment**
- âœ… **Structured format**: Consistent JSON structure
- âœ… **Tactical accuracy**: Generated by GPT-4 with fighter pilot doctrine
- âœ… **Scenario diversity**: Multiple engagement ranges and conditions
- âŒ **Limited quantity**: Only 200+ samples (need 1000+ for effective fine-tuning)
- âŒ **Single domain**: Only air-to-air combat (missing CAS, SEAD, etc.)

---

## ðŸš€ **Proposed Fine-Tuning Strategy**

### **Phase 1: Data Expansion and Enrichment**

#### **1.1 Expand Existing Dataset**
```python
# Implement in: llm/data_expansion.py
class TacticalDataExpander:
    def expand_scenarios(self, base_scenarios: List[Dict], target_count: int = 5000):
        """
        Expand existing 200 scenarios to 5000+ using:
        - Parameter variation (distance, altitude, heading combinations)
        - Weather conditions (clear, clouds, rain, night)
        - Threat environments (single, multiple, SEAD)
        - Aircraft types (F-16, F-18, F-35, Eurofighter vs various threats)
        - Weapons loadouts (different missile combinations)
        """
        
    def add_domain_variations(self, scenarios: List[Dict]):
        """
        Add tactical domain variations:
        - BVR scenarios (>50km engagements)
        - WVR scenarios (<10km dogfights)
        - Multi-target scenarios
        - Defensive scenarios (being targeted)
        - Support scenarios (CAS, SEAD)
        """
```

#### **1.2 Create Harfang-Specific Training Data**
```python
# Implement in: llm/harfang_data_generator.py
class HarfangTacticalDataGenerator:
    def generate_harfang_scenarios(self, num_scenarios: int = 2000):
        """
        Generate Harfang-specific training data using:
        - Actual Harfang environment states (25-dimensional)
        - Real engagement trajectories from trained agents
        - Expert pilot annotations on successful/failed engagements
        - Multi-stage reasoning examples (Strategic/Tactical/Execution)
        """
        
    def create_multi_stage_examples(self, base_scenarios: List[Dict]):
        """
        Convert single-stage scenarios to multi-stage reasoning:
        - Strategic: Mission planning and long-term objectives
        - Tactical: Engagement geometry and immediate priorities
        - Execution: Control input critique and micro-adjustments
        """
```

### **Phase 2: Model Selection and Fine-Tuning Approach**

#### **2.1 Recommended Base Models for Fine-Tuning**
1. **Primary Choice: Llama 3.1 8B**
   - Excellent reasoning capabilities
   - Good fine-tuning performance
   - Manageable size for training
   - Strong instruction following

2. **Alternative: Gemma 2 9B**
   - Fast inference
   - Good fine-tuning stability
   - Efficient memory usage

3. **Research Option: Mixtral 8x7B**
   - Mixture of experts architecture
   - Excellent for specialized domains
   - Higher quality but more resource intensive

#### **2.2 Fine-Tuning Methodology**

**Option A: LoRA Fine-Tuning (Recommended)**
```python
# Implement in: llm/lora_finetuning.py
class CombatLoRATrainer:
    """
    Low-Rank Adaptation fine-tuning for combat LLMs
    
    Advantages:
    - Much faster training (hours vs days)
    - Lower memory requirements
    - Can be done on single GPU
    - Easy to merge with base model
    """
    
    def setup_lora_training(self, base_model: str = "llama3.1:8b"):
        config = {
            'r': 16,                    # LoRA rank
            'lora_alpha': 32,           # LoRA scaling
            'target_modules': ['q_proj', 'v_proj', 'o_proj', 'gate_proj'],
            'lora_dropout': 0.1,
            'bias': 'none',
            'task_type': 'CAUSAL_LM'
        }
        
    def train_combat_lora(self, training_data: List[Dict], epochs: int = 3):
        """
        Train LoRA adapter for combat tactical reasoning
        
        Training format:
        - Input: Tactical situation description
        - Output: Structured tactical response
        - Loss: Cross-entropy on tactical decision tokens
        """
```

**Option B: Full Fine-Tuning (Advanced)**
```python
# Implement in: llm/full_finetuning.py  
class CombatFullTrainer:
    """
    Full model fine-tuning for maximum customization
    
    Advantages:
    - Complete model adaptation
    - Maximum performance potential
    - Full control over model behavior
    
    Disadvantages:
    - Requires significant compute (multiple GPUs)
    - Longer training time (days/weeks)
    - Higher memory requirements
    """
```

#### **2.3 Training Data Format**
```json
{
    "instruction": "You are an expert fighter pilot providing tactical guidance.",
    "input": "Tactical Situation: Range 8500m, Enemy climbing, Radar locked, BVR engagement phase, High energy state, Aspect angle 15Â°, Closure rate 250m/s. Recommend immediate action.",
    "output": {
        "strategic_assessment": "Maintain BVR advantage, prepare for long-range missile engagement",
        "tactical_recommendation": "Fire AIM-120C at maximum effective range, maintain energy advantage",
        "execution_guidance": "Slight climb to maintain energy, prepare for defensive maneuvers post-shot",
        "shaping_delta": 0.3,
        "confidence": 0.9
    }
}
```

### **Phase 3: Specialized Model Creation**

#### **3.1 Create Domain-Specific Models**
```python
# Implement in: llm/specialized_models.py
class SpecializedCombatModels:
    """
    Create specialized models for different combat aspects:
    
    1. BVR_Specialist: Beyond Visual Range engagement expert
    2. ACM_Specialist: Air Combat Maneuvering (dogfight) expert  
    3. Defensive_Specialist: Defensive tactics and evasion expert
    4. Multi_Target_Specialist: Multiple threat engagement expert
    """
    
    def create_bvr_specialist(self, base_model: str):
        """Fine-tune specifically for BVR engagements (>15km)"""
        
    def create_acm_specialist(self, base_model: str):
        """Fine-tune specifically for close-range dogfighting (<5km)"""
        
    def create_defensive_specialist(self, base_model: str):
        """Fine-tune specifically for defensive scenarios"""
```

---

## ðŸ›  **Implementation Recommendations**

### **Immediate Actions (Next 1-2 weeks)**

1. **Expand Training Dataset**
   - Generate 2000+ additional tactical scenarios
   - Create multi-stage reasoning examples
   - Add Harfang-specific state mappings

2. **Setup LoRA Fine-Tuning Pipeline**
   - Implement LoRA training infrastructure
   - Create training data preprocessing
   - Setup evaluation metrics

3. **Benchmark Current Models**
   - Test all supported models on tactical reasoning
   - Identify best base model for fine-tuning
   - Measure baseline performance

### **Medium-term Goals (1-2 months)**

1. **Train Specialized Models**
   - Fine-tune BVR specialist
   - Fine-tune ACM specialist
   - Create ensemble system

2. **Integration with RL Training**
   - Integrate fine-tuned models into training loop
   - Compare performance vs generic models
   - Optimize for real-time inference

### **Advanced Features (2-3 months)**

1. **Continuous Learning System**
   - Online fine-tuning during RL training
   - Adaptive model selection based on scenario
   - Performance-based model switching

2. **Multi-Model Ensemble**
   - Combine multiple specialized models
   - Weighted voting based on scenario type
   - Confidence-based model selection

---

## ðŸ’° **Resource Requirements**

### **For LoRA Fine-Tuning (Recommended)**
- **Hardware**: Single GPU (RTX 3080/4080 or better)
- **Memory**: 16GB+ RAM, 8GB+ VRAM
- **Training Time**: 4-8 hours per model
- **Storage**: 50GB+ for datasets and models

### **For Full Fine-Tuning (Advanced)**
- **Hardware**: Multiple GPUs (4x RTX 4090 or better)
- **Memory**: 64GB+ RAM, 32GB+ VRAM
- **Training Time**: 1-2 weeks per model
- **Storage**: 200GB+ for full models

---

## ðŸ“ˆ **Expected Performance Improvements**

### **Baseline (Generic LLM)**
- Tactical accuracy: ~60-70%
- Response relevance: ~70-80%
- Combat doctrine adherence: ~50-60%

### **After LoRA Fine-Tuning**
- Tactical accuracy: ~80-90%
- Response relevance: ~90-95%
- Combat doctrine adherence: ~85-95%
- Response consistency: +40%
- Domain-specific knowledge: +60%

### **After Full Fine-Tuning**
- Tactical accuracy: ~90-95%
- Response relevance: ~95-98%
- Combat doctrine adherence: ~95-99%
- Response consistency: +50%
- Domain-specific knowledge: +80%

---

## ðŸŽ¯ **Recommended Implementation Order**

1. **Week 1**: Expand dataset to 2000+ scenarios
2. **Week 2**: Setup LoRA fine-tuning pipeline
3. **Week 3**: Train BVR specialist model
4. **Week 4**: Train ACM specialist model
5. **Week 5**: Integration and testing
6. **Week 6**: Performance evaluation and optimization

---

## â“ **Key Questions for Decision**

1. **Resource Availability**: Do you have access to GPU for fine-tuning?
2. **Training Time**: Are you willing to invest 1-2 weeks for specialized models?
3. **Complexity**: Start with LoRA (faster) or full fine-tuning (better)?
4. **Specialization**: Single general model or multiple specialist models?
5. **Data Generation**: Should we generate more training data first?

**My Recommendation**: Start with **LoRA fine-tuning on Llama 3.1 8B** using expanded dataset. This provides the best balance of performance improvement, resource requirements, and implementation speed.
